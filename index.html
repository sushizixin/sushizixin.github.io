<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zixin Guo</title>

  <meta name="author" content="Zixin Guo">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="images/favicon/favicon.svg" type="image/svg+xml">
  <link rel="apple-touch-icon" href="images/favicon/apple-touch-icon.png">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <!-- ==================== Bio Section ==================== -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Zixin Guo
                  </p>
                  <p>
                    I'm a fourth-year PhD student at Department of Computer Science, <a href="https://www.aalto.fi/">Aalto
                      University</a>, where I am advised by <a
                      href="https://www.aalto.fi/en/people/jorma-laaksonen">Jorma Laaksonen</a>. My current research
                    interests
                    include:
                  </p>
                  <ul>
                    <li>Multimodal Learning</li>
                    <li>Eye Tracking</li>
                  </ul>
                  <p style="text-align:center">
                    <a href="mailto:zixin.guo@aalto.fi">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=YOUR_SCHOLAR_ID">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/sushizixin">Github</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/zixin-guo-71596a178/?originalSubdomain=fi">LinkedIn</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/profile.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/profile.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- ==================== Selected Publications ==================== -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Selected Publications</h2>
                  <p>
                    * = equal contribution &nbsp;&nbsp; # = I mentored
                  </p>

                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- Imagine How To Change (ICLR 2026) -->
              <tr>

                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">Imagine How To Change: Explicit Procedure Modeling for Change
                    Captioning</span>
                  <br>
                  Jiayang Sun*,
                  <strong>Zixin Guo*#</strong>,
                  Min Cao,
                  Guibo Zhu,
                  Jorma Laaksonen
                  <br>
                  <em>ICLR</em>, 2026
                  <br>
                  <a href="https://openreview.net/forum?id=1lK8LPFcXp">paper</a>
                  /
                  <a href="https://github.com/BlueberryOreo/ProCap">code</a>
                </td>
              </tr>

              <!-- SeekUI (CHI 2026) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">SeekUI: Predicting Visual Search Behavior on Graphical User Interfaces with a
                    Reward-Augmented Vision Language Model</span>
                  <br>
                  <strong>Zixin Guo*</strong>,
                  Yue Jiang*,
                  Luis A. Leiva,
                  Antti Oulasvirta
                  <br>
                  <em>CHI</em>, 2026
                  <br>
                  <a href="https://github.com/YueJiang-nj/SeekUI-CHI2026">code</a>
                </td>
              </tr>

              <!-- Learning to Describe Implicit Changes (EMNLP Findings 2025) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">Learning to Describe Implicit Changes: Noise-Robust Pre-training for Image
                    Difference Captioning</span>
                  <br>
                  <strong>Zixin Guo</strong>,
                  Jiayang Sun,
                  Tzu-Jui Julius Wang,
                  Abduljalil Radman,
                  Selen Pehlivan,
                  Min Cao,
                  Jorma Laaksonen
                  <br>
                  <em>Findings of EMNLP</em>, 2025
                  <br>
                  <a href="https://doi.org/10.18653/v1/2025.findings-emnlp.537">paper</a>
                </td>
              </tr>

              <!-- EyeFormer (UIST 2024) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">EyeFormer: Predicting Scanpaths in Free-Viewing Tasks with Transformer-Guided
                    Reinforcement Learning</span>
                  <br>
                  Yue Jiang*,
                  <strong>Zixin Guo*</strong>,
                  Hamed Rezazadegan Tavakoli,
                  Luis A. Leiva,
                  Antti Oulasvirta
                  <br>
                  <em>UIST</em>, 2024
                  <br>
                  <a href="https://doi.org/10.1145/3654777.3676436">paper</a>
                  /
                  <a href="https://github.com/YueJiang-nj/EyeFormer-UIST2024">code</a>
                </td>
              </tr>

              <!-- PiTL (SIGIR 2023) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language
                    Pre-training via Prompting</span>
                  <br>
                  <strong>Zixin Guo</strong>,
                  Tzu-Jui Julius Wang,
                  Selen Pehlivan,
                  Abduljalil Radman,
                  Jorma Laaksonen
                  <br>
                  <em>SIGIR</em>, 2023 &nbsp<font color="gray"><strong>(Short Paper)</strong></font>
                  <br>
                  <a href="https://doi.org/10.1145/3539618.3592038">paper</a>
                </td>
              </tr>

              <!-- CLIP4IDC (AACL 2022) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">CLIP4IDC: CLIP for Image Difference Captioning</span>
                  <br>
                  <strong>Zixin Guo</strong>,
                  Tzu-Jui Julius Wang,
                  Jorma Laaksonen
                  <br>
                  <em>AACL</em>, 2022 &nbsp<font color="gray"><strong>(Short Paper)</strong></font>
                  <br>
                  <a href="https://doi.org/10.18653/v1/2022.aacl-short.5">paper</a>
                  /
                  <a href="https://github.com/sushizixin/CLIP4IDC">code</a>
                </td>
              </tr>

            </tbody>
          </table>

          <!-- ==================== Other Publications ==================== -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Other Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- Valor32k-AVQA v2.0 (ACM MM 2025) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">Valor32k-AVQA v2.0: Open-Ended Audio-Visual Question Answering Dataset and
                    Benchmark</span>
                  <br>
                  Ines Riahi,
                  Abduljalil Radman,
                  <strong>Zixin Guo</strong>,
                  Rachid Hedjam,
                  Jorma Laaksonen
                  <br>
                  <em>ACM MM</em>, 2025
                  <br>
                  <a href="https://doi.org/10.1145/3746027.3758261">paper</a>
                  /
                  <a href="https://github.com/inesriahi/valor32k-avqa-2">code</a>
                  /
                  <a href="https://inesriahi.github.io/valor32k-avqa-2/">project</a>
                </td>
              </tr>

              <!-- Prompt-based Weakly-supervised VLP (PRL 2025) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">Prompt-based Weakly-supervised Vision-language Pre-training</span>
                  <br>
                  <strong>Zixin Guo</strong>,
                  Tzu-Jui Julius Wang,
                  Selen Pehlivan,
                  Abduljalil Radman,
                  Min Cao,
                  Jorma Laaksonen
                  <br>
                  <em>Pattern Recognition Letters</em>, 2025
                  <br>
                  <a href="https://doi.org/10.1016/j.patrec.2025.06.020">paper</a>
                </td>
              </tr>

              <!-- Efficient Text-to-video Retrieval (Visual Intelligence 2025) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">Efficient Text-to-video Retrieval via Multi-modal Multi-tagger Derived
                    Pre-screening</span>
                  <br>
                  Yingjia Xu,
                  Mengxia Wu,
                  <strong>Zixin Guo</strong>,
                  Min Cao,
                  Mang Ye,
                  Jorma Laaksonen
                  <br>
                  <em>Visual Intelligence</em>, 2025
                  <br>
                  <a href="https://doi.org/10.1007/s44267-025-00073-2">paper</a>
                </td>
              </tr>

              <!-- Diffusion-based Multimodal Video Captioning (ACCV 2024) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">Diffusion-based Multimodal Video Captioning</span>
                  <br>
                  Jaakko Kainulainen,
                  <strong>Zixin Guo#</strong>,
                  Jorma Laaksonen
                  <br>
                  <em>ACCV</em>, 2024
                  <br>
                  <a href="https://doi.org/10.1007/978-981-96-0908-6_9">paper</a>
                </td>
              </tr>

              <!-- Impact of Design Decisions in Scanpath Modeling (ETRA 2024) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">Impact of Design Decisions in Scanpath Modeling</span>
                  <br>
                  Parvin Emami,
                  Yue Jiang,
                  <strong>Zixin Guo</strong>,
                  Luis A. Leiva
                  <br>
                  <em>ETRA</em>, 2024
                  <br>
                  <a href="https://doi.org/10.1145/3655602">paper</a>
                  /
                  <a href="https://github.com/prviin/scanpath-design-decisions">code</a>
                </td>
              </tr>

              <!-- Post-Attention Modulator (ICPR 2022) -->
              <tr>
                <td style="padding:16px 20px;width:100%;vertical-align:middle">
                  <span class="papertitle">Post-Attention Modulator for Dense Video Captioning</span>
                  <br>
                  <strong>Zixin Guo</strong>,
                  Tzu-Jui Julius Wang,
                  Jorma Laaksonen
                  <br>
                  <em>ICPR</em>, 2022
                  <br>
                  <a href="https://doi.org/10.1109/ICPR56361.2022.9956260">paper</a>
                </td>
              </tr>

            </tbody>
          </table>

          <!-- ==================== Footer ==================== -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Website template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
